## Классификация методов безусловной оптимизации. Скорости сходимости. Методы первого порядка. Градиентные методы. Методы второго порядка. Метод Ньютона и его модификации. Квазиньютоновские методы. Методы переменной метрики. Методы сопряженных градиентов. Конечно-разностнаяаппроксимация производных. Конечно-разностные методы. Методы нулевого порядка. Методы покоординатного спуска, Хука—Дживса, сопряженных направлений. Методы деформируемых конфигураций. Симплексные методы. Комплекс-методы. Решение задач многокритериальной оптимизации методами прямого поиска. 
---
## Классификация методов безусловной оптимизации. 

Задача безусловной оптимизации состоит в нахождении минимума или
максимума функции в отсутствие каких-либо ограничений. Несмотря на то что
большинство практических задач оптимизации содержит ограничения,
изучение методов безусловной оптимизации важно с нескольких точек
зрения. Многие алгоритмы решения задачи с ограничениями предполагают
сведение ее к последовательности задач безусловной оптимизации. Другой
класс методов основан на поиске подходящего направления и последующей
минимизации вдоль этого направления. Обоснование методов безусловной
оптимизации может быть естественным образом распространено на
обоснование процедур решения задач с ограничениями.

Методы оптимизации можно разделить на:
- методы нулевого порядка (анализ значения целевой ф-ции)
- методы первого порядка (анализ значения 1-ой производной)
- методы второго порядка (анализ значения 2-ой производной)

Методы нулевого порядка однопараметрическая функция
- равномерный поиск
- метод деления пополам
- метод Фибоначи
- метод золотого сечения
- метод случайного поиска
- направление случайного поиска
- метод Гаусса (метод покардинатоного поиска)
Все это методы можно использовать и для нескольких стартовых точек (т е из нескольких локальных оптимумов выбирается один)
- симплекс метод

## Скорости сходимости. 
https://ru.wikipedia.org/wiki/Скорость_сходимости
Пусть {\displaystyle \left\{x_{n}\right\}}{\displaystyle \left\{x_{n}\right\}} — сходящаяся последовательность приближений некоторого алгоритма нахождения корня уравнения или экстремума функции {\displaystyle x^{*}}x^{*}, тогда:
Говорят, что метод обладает линейной сходимостью, если {\displaystyle \exists \alpha \in (0,\;1):\quad \exists N\in \mathbb {N} ,\;\forall n\geq N\quad ||x_{n}-x^{*}||<\alpha ||x_{n-1}-x^{*}||}{\displaystyle \exists \alpha \in (0,\;1):\quad \exists N\in \mathbb {N} ,\;\forall n\geq N\quad ||x_{n}-x^{*}||<\alpha ||x_{n-1}-x^{*}||}.
Говорят, что метод обладает сходимостью степени {\displaystyle \beta }\beta , если {\displaystyle \exists \alpha \in (0,\;1]:\quad \exists N\in \mathbb {N} ,\;\forall n\geq N\quad ||x_{n}-x^{*}||<\alpha ||x_{n-1}-x^{*}||^{\beta }}{\displaystyle \exists \alpha \in (0,\;1]:\quad \exists N\in \mathbb {N} ,\;\forall n\geq N\quad ||x_{n}-x^{*}||<\alpha ||x_{n-1}-x^{*}||^{\beta }}.
Отметим, что обычно скорость сходимости методов не превышает квадратичной. В редких случаях метод может обладать кубической скоростью сходимости

## Методы первого порядка. 
## Градиентные методы. 
Градиентные методы — численные методы решения с помощью градиента задач, сводящихся к нахождению экстремумов функции.

## Методы второго порядка. 
## Метод Ньютона и его модификации. 
http://www.machinelearning.ru/wiki/index.php?title=Метод_Ньютона._Проблема_области_сходимости._Метод_парабол._Совмещение_методов_Ньютона_и_парабол
http://e-lib.gasu.ru/eposobia/metody/R_2_4.html

Метод секущих для нелинейного уравнения.
Метод хорд для нелинейного уравнения.
Упрощенный метод Ньютона.
Модификация метода Ньютона для системы двух уравнений.


## Квазиньютоновские методы. 
Квазиньютоновские методы — методы оптимизации, основанные на накоплении информации о кривизне целевой функции по наблюдениям за изменением градиента, чем принципиально отличаются от ньютоновских методов. Класс квазиньютоновских методов исключает явное формирование матрицы Гессе, заменяя её некоторым приближением.

## Методы переменной метрики. 

Наибольшей эффективностью поиска характеризуются методы второго порядка. Однако они требуют вычисления матрицы Гессе, что значительно затрудняет или даже делает невозможным их использование для решения конкретных задач оптимизации. В этой связи были разработаны квазинью- тоновские методы, в которых осуществляется аппроксимация матрицы Гессе. Итерационная формула квазиньютоновского алгоритма такая же, как в любом градиентном методе (см. формулу (12.110)), а направление поиска при минимизации целевой функции определяется вектором

где Dk — квадратная матрица порядка п, аппроксимирующая обращенную матрицу Гессе в формуле Ньютона (12.80); gk — единичный вектор, определяющий направление вектор-градиента в точке Xk.

Матрица Dk в формуле (12.111) носит название метрики. Методы поиска вдоль направлений, определяемых этой формулой, называются методами переменной метрики, поскольку матрица Dk изменяется на каждой итерации. Так как методы переменной метрики не используют вторых производных целевой функции, то они относятся к методам первого порядка.

Алгоритм метода переменной метрики включает следующие этапы:

Этап 1. Задание параметров алгоритма , е и начальной точки поиска Х0.
Этап 2. Определение целевой функции и ее градиента в точке Х0.
Этап 3. Задание начального значения аппроксимирующей матрицыD0 = Е.
Этап 4. Определение вектора направления поиска Sk по формуле (12.111).
Этап 5. Определение шага поиска hk на основе одномерной минимизации функции F(X) в направлении вектора Sk.
Этап 6. Вычисление координат новой отображающей точки по формуле
(12.110).
Этап 7. Определение градиента целевой функции в полученной точке.
Этап 8. Проверка условий окончания поиска. Если неравенства (12.69) и (12.70) (или хотя бы одно из них) удовлетворяются, поиск завершается. В противном случае переходят к этапу 9.
Этап 9. Проверка необходимости обновления матрицы Dk. При k = {п, 2п, 3п, ...} переходят к этапу 3, а в противном случае — к этапу 10.
Этап 10. Определение матрицы Dk по формулам (12.112)-(12.115).


## Методы сопряженных градиентов. 
http://www.machinelearning.ru/wiki/index.php?title=Метод_сопряжённых_градиентов
Метод сопряжённых градиентов — итерационный метод для безусловной оптимизации в многомерном пространстве. Основным достоинством метода является то, что он решает квадратичную задачу оптимизации за конечное число шагов. Поэтому, сначала описывается метод сопряжённых градиентов для оптимизации квадратичного функционала, выводятся итерационные формулы, приводятся оценки скорости сходимости. После этого показывается, как метод сопряжённых обобщается для оптимизации произвольного функционала, рассматриваются различные варианты метода, обсуждается сходимость.

## Конечно-разностнаяаппроксимация производных. 
## Конечно-разностные методы. 

## Методы нулевого порядка. 
В этих методах для определения направления спуска не требуется вычислять производные целевой функции. Направление минимизации в данном случае полностью определяется последовательными вычислениями значений функции. Следует отметить, что при решении задач безусловной минимизации методы первого и второго порядков обладают, как правило, более высокой скоростью сходимости, чем методы нулевого порядка. Однако на практике вычисление первых и вторых производных функции большого количества переменных весьма трудоемко. В ряде случаев они не могут быть получены в виде аналитических функций. Определение производных с помощью различных численных методов осуществляется с ошибками, которые могут ограничить применение таких методов. Кроме того, на практике встречаются задачи, решение которых возможно лишь с помощью методов нулевого порядка, например задачи минимизации функций с разрывными первыми производными. Критерий оптимальности может быть задан не в явном виде, а системой уравнений. В этом случае аналитическое или численное определение производных становится очень сложным, а иногда невозможным. Для решения таких практических задач оптимизации могут быть успешно применены методы нулевого порядка. Рассмотрим некоторые из них.

## Методы покоординатного спуска, Хука—Дживса, сопряженных направлений. 

## Метод прямого поиска (метод Хука-Дживса)
Суть этого метода состоит в следующем. Задаются некоторой
начальной точкой х[0]. Изменяя компоненты вектора х[0], обследуют
окрестность данной точки, в результате чего находят направление, в
котором происходит уменьшение минимизируемой функции f(x). В
выбранном направлении осуществляют спуск до тех пор, пока
значение функции уменьшается. После того как в данном направлении
не удается найти точку с меньшим значением функции, уменьшают
величину шага спуска. Если последовательные дробления шага не
приводят к уменьшению функции, от выбранного направления спуска
отказываются и осуществляют новое обследование окрестности и т. д

## Методы деформируемых конфигураций. 
https://ru.wikipedia.org/wiki/Метод_Нелдера_—_Мида

Пусть требуется найти безусловный минимум функции n переменных {\displaystyle f\left(x^{(1)},x^{(2)},\ldots ,x^{(n)}\right)}f\left(x^{{(1)}},x^{{(2)}},\ldots ,x^{{(n)}}\right). Предполагается, что серьёзных ограничений на область определения функции нет, то есть функция определена во всех встречающихся точках.

Параметрами метода являются:

коэффициент отражения {\displaystyle \alpha >0}\alpha >0, обычно выбирается равным {\displaystyle 1}1.
коэффициент сжатия {\displaystyle \beta >0}\beta >0, обычно выбирается равным {\displaystyle 0{,}5}0{,}5.
коэффициент растяжения {\displaystyle \gamma >0}\gamma >0, обычно выбирается равным {\displaystyle 2}2.
«Подготовка». Вначале выбирается {\displaystyle n+1}n+1 точка {\displaystyle x_{i}=\left(x_{i}^{(1)},x_{i}^{(2)},\ldots ,x_{i}^{(n)}\right),i=1..n+1}x_{i}=\left(x_{i}^{{(1)}},x_{i}^{{(2)}},\ldots ,x_{i}^{{(n)}}\right),i=1..n+1, образующие симплекс n-мерного пространства. В этих точках вычисляются значения функции: {\displaystyle f_{1}=f(x_{1}),f_{2}=f(x_{2}),\ldots ,f_{n+1}=f(x_{n+1})}f_{1}=f(x_{1}),f_{2}=f(x_{2}),\ldots ,f_{{n+1}}=f(x_{{n+1}}).
«Сортировка». Из вершин симплекса выбираем три точки: {\displaystyle x_{h}}x_{h} с наибольшим (из выбранных) значением функции {\displaystyle f_{h}}f_{h}, {\displaystyle x_{g}}x_{g} со следующим по величине значением {\displaystyle f_{g}}f_{g} и {\displaystyle x_{l}}x_{l} с наименьшим значением функции {\displaystyle f_{l}}f_{l}. Целью дальнейших манипуляций будет уменьшение по крайней мере {\displaystyle f_{h}}f_{h}.
Найдём центр тяжести всех точек, за исключением {\displaystyle x_{h}}x_{h}: {\displaystyle x_{c}={\frac {1}{n}}\sum \limits _{i\neq h}x_{i}}x_{c}={\frac  {1}{n}}\sum \limits _{{i\neq h}}x_{i}. Вычислять {\displaystyle f_{c}=f(x_{c})}f_{c}=f(x_{c}) не обязательно.
«Отражение». Отразим точку {\displaystyle x_{h}}x_{h} относительно {\displaystyle x_{c}}x_{c} с коэффициентом {\displaystyle \alpha }\alpha  (при {\displaystyle \alpha =1}\alpha =1 это будет центральная симметрия, в общем случае — гомотетия), получим точку {\displaystyle x_{r}}x_{r} и вычислим в ней функцию: {\displaystyle f_{r}=f(x_{r})}f_{r}=f(x_{r}). Координаты новой точки вычисляются по формуле:
{\displaystyle x_{r}=(1+\alpha )x_{c}-\alpha x_{h}}x_{r}=(1+\alpha )x_{c}-\alpha x_{h}.
Далее смотрим, насколько нам удалось уменьшить функцию, ищем место {\displaystyle f_{r}}f_{r} в ряду {\displaystyle f_{h},f_{g},f_{l}}f_{h},f_{g},f_{l}.
Если {\displaystyle f_{r}<f_{l}}f_{r}<f_{l}, то направление выбрано удачное и можно попробовать увеличить шаг. Производим «растяжение». Новая точка {\displaystyle x_{e}=(1-\gamma )x_{c}+\gamma x_{r}}x_{e}=(1-\gamma )x_{c}+\gamma x_{r} и значение функции {\displaystyle f_{e}=f(x_{e})}f_{e}=f(x_{e}).
Если {\displaystyle f_{e}<f_{r}}f_{e}<f_{r}, то можно расширить симплекс до этой точки: присваиваем точке {\displaystyle x_{h}}x_{h} значение {\displaystyle x_{e}}x_{e} и заканчиваем итерацию (на шаг 9).
Если {\displaystyle f_{r}<f_{e}}f_{r}<f_{e}, то переместились слишком далеко: присваиваем точке {\displaystyle x_{h}}x_{h} значение {\displaystyle x_{r}}x_{r} и заканчиваем итерацию (на шаг 9).
Если {\displaystyle f_{l}<f_{r}<f_{g}}f_{l}<f_{r}<f_{g}, то выбор точки неплохой (новая лучше двух прежних). Присваиваем точке {\displaystyle x_{h}}x_{h} значение {\displaystyle x_{r}}x_{r} и переходим на шаг 9.
Если {\displaystyle f_{g}<f_{r}<f_{h}}f_{g}<f_{r}<f_{h}, то меняем местами значения {\displaystyle x_{r}}x_{r} и {\displaystyle x_{h}}x_{h}. Также нужно поменять местами значения {\displaystyle f_{r}}f_{r} и {\displaystyle f_{h}}f_{h}. После этого идём на шаг 6.
Если {\displaystyle f_{h}<f_{r}}f_{h}<f_{r}, то просто идём на следующий шаг 6.
В результате (возможно, после переобозначения) {\displaystyle f_{l}<f_{g}<f_{h}<f_{r}}f_{l}<f_{g}<f_{h}<f_{r}.
«Сжатие». Строим точку {\displaystyle x_{s}=\beta x_{h}+(1-\beta )x_{c}}x_{s}=\beta x_{h}+(1-\beta )x_{c} и вычисляем в ней значение {\displaystyle f_{s}=f(x_{s})}f_{s}=f(x_{s}).
Если {\displaystyle f_{s}<f_{h}}f_{s}<f_{h}, то присваиваем точке {\displaystyle x_{h}}x_{h} значение {\displaystyle x_{s}}x_{s} и идём на шаг 9.
Если {\displaystyle f_{s}>f_{h}}f_{s}>f_{h}, то первоначальные точки оказались самыми удачными. Делаем «глобальное сжатие» симплекса — гомотетию к точке с наименьшим значением {\displaystyle x_{l}}x_{l}:
{\displaystyle x_{i}\gets x_{l}+(x_{i}-x_{l})/2}x_{i}\gets x_{l}+(x_{i}-x_{l})/2, {\displaystyle i\neq l}i\neq l.
Последний шаг — проверка сходимости. Может выполняться по-разному, например, оценкой дисперсии набора точек. Суть проверки заключается в том, чтобы проверить взаимную близость полученных вершин симплекса, что предполагает и близость их к искомому минимуму. Если требуемая точность ещё не достигнута, можно продолжить итерации с шага 2.

## Симплексные методы. 
http://ecat.diit.edu.ua/ft/Optimization2_1.pdf

## Комплекс-методы. 
https://studfile.net/preview/1676074/page:2/
При решении задачи (1) методом комплексов используются следующие операции:
генерация случайного комплекса;
отражение вершины комплекса с растяжением;
сжатие комплекса.
Задаем начальную точку , исходя из которой должен быть построен комплекс , начальное значение величины и полагаем счетчик числа итераций.
Генерируем N случайных векторов и по формуле (2) находим координаты вершин комплекса .
Вычисляем величины ,.
Находим максимальную из величин ,:

По формуле (3) отражаем вершину комплекса- получаем вершинуи новый комплекс. Вычисляем величину.
Если <, то переходим к п. 7. Иначе – по формуле (5) выполняем сжатие симплексав направлении, получаем вершинуи новый комплекс, полагаеми переходим к п. 4.
Если условие окончания поиска выполнено (см. ниже), то в качестве точки принимаем вершину комплекса , к которой функция Ф(Х) имеет наименьшее значение и завершаем итерации. Иначе – полагаем переходим к п. 4●

В качестве критерия окончания поиска могут использоваться следующие условия:

максимальная длина ребра комплекса не превышает - требуемую точность решения поX;

максимальная разность значений функции Ф(Х) в двух вершинах комплекса не превышает - требуемую точность решения поФ.

## Решение задач многокритериальной оптимизации методами прямого поиска. 

Этот метод является методом нулевого порядка. Суть этого метода состоит в следующем. Задаются некоторой начальной точкой х(0). Изменяя компоненты вектора х(0 обследуют окрестность данной точки, в результате чего находят направление, в котором происходит уменьшение функции /(х), и величину шага спуска. Осуществляют спуск к новому значению приближения к точке минимума функции. Затем процесс повторяется уже для этого приближения. Алгоритм метода прямого поиска состоит в следующем.

1. Зададим значения координат х^, /7=1, ..., N, начальной точки х(0 величину изменения координат /z0, точность приближенного решения г. Положим к = 0.
2. Положим h = /г0.
3. Будем циклически изменять каждую координату точки х(к) на величину h, т. е. х* = х^ + h;x* = xjfi - hfn= 1,..., N. При этом вычисляют значения /(х*) и сравнивают их со значением /(x(/l)). Если /(х*) < /(х<*>), то соответствующая координата х*уп= 1,..., N, приобретает новое значение, вычисленное по одному из приведенных выражений. В противном случае значение этой координаты остается неизменным.
4. Если после изменения последней п-й координаты /(х*) h < s. Если оно выполняется, то вычисления прекращаются. Иначе полагаем h «— ah (а < 1) и переходим к п. 3.
5. Осуществляем спуск к повой точке: = х*. Вычисляем значение/(х(А+1)). Если |/(х(А+1)) -/(х(А))| < 8, то вычисления прекращаются и х(/’ ** — вычисленное положение минимума. В противном случае полагаем к к + 1 и переходим к п. 2.
 
< Пред	 	СОДЕРЖАНИЕ	 	ОРИГИНАЛ	 	 	След >
Метод прямого поиска

Этот метод является методом нулевого порядка. Суть этого метода состоит в следующем. Задаются некоторой начальной точкой х(0). Изменяя компоненты вектора х(0 обследуют окрестность данной точки, в результате чего находят направление, в котором происходит уменьшение функции /(х), и величину шага спуска. Осуществляют спуск к новому значению приближения к точке минимума функции. Затем процесс повторяется уже для этого приближения. Алгоритм метода прямого поиска состоит в следующем.

1. Зададим значения координат х^, /7=1, ..., N, начальной точки х(0 величину изменения координат /z0, точность приближенного решения г. Положим к = 0.
2. Положим h = /г0.
3. Будем циклически изменять каждую координату точки х(к) на величину h, т. е. х* = х^ + h;x* = xjfi - hfn= 1,..., N. При этом вычисляют значения /(х*) и сравнивают их со значением /(x(/l)). Если /(х*) < /(х<*>), то соответствующая координата х*уп= 1,..., N, приобретает новое значение, вычисленное по одному из приведенных выражений. В противном случае значение этой координаты остается неизменным.
4. Если после изменения последней п-й координаты /(х*) h < s. Если оно выполняется, то вычисления прекращаются. Иначе полагаем h «— ah (а < 1) и переходим к п. 3.
Осуществляем спуск к повой точке: = х*. Вычисляем значение/(х(А+1)). Если |/(х(А+1)) -/(х(А))| < 8, то вычисления прекращаются и х(/’ ** — вычисленное положение минимума. В противном случае полагаем к к + 1 и переходим к п. 2.

Достоинством метода прямого поиска является его простота. Он не требует знания целевой функции в явном виде, а также легко учитывает ограничения на отдельные переменные и сложные ограничения на область поиска. Недостаток метода прямого поиска состоит в том, что в случае сильно вытянутых, изогнутых или обладающих острыми углами линий уровня целевой функции он может не обеспечивать продвижение к точке минимума.

Существуют и другие методы нулевого порядка, такие как метод деформируемого многогранника, метод вращающихся координат, метод параллельных касательных.